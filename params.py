# Learning rate for optimizers
lr = 0.0002

# Beta1 hyperparam for Adam optimizers
beta1 = 0.9

# Training min-batch size
batch_size = 16

num_epochs = 8
dataset_path = '/home/aftaab/Datasets/celeba-dataset/'